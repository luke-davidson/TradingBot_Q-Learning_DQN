{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Environment"
      ],
      "metadata": {
        "collapsed": false,
        "id": "XUuFvD_NgQgr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "collapsed": true,
        "id": "nEsslYFDgQgt"
      },
      "outputs": [],
      "source": [
        "import dataclasses\n",
        "import os\n",
        "from cmath import inf\n",
        "from collections import namedtuple\n",
        "from copy import deepcopy\n",
        "from enum import Enum\n",
        "from pathlib import Path\n",
        "from typing import Any, List\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from gym import spaces\n",
        "from gym.utils import seeding\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "BaseEnvTimestep = namedtuple('BaseEnvTimestep', ['obs', 'reward', 'done', 'info'])\n",
        "\n",
        "\n",
        "def load_dataset(name, index_name):\n",
        "    # base_dir = os.path.dirname(os.path.abspath('STOCKS_GOOGL.csv'))\n",
        "    base_dir = os.path.dirname(os.path.abspath('FOREX_EURUSD_1H_ASK.csv'))\n",
        "    path = os.path.join(base_dir, name + '.csv')\n",
        "    assert os.path.exists(\n",
        "        path\n",
        "    ), \"You need to put the stock data under the \\'environment/envs/data\\' folder.\\n \\\n",
        "        if using StocksEnv, you can download Google stocks data at \\\n",
        "        https://github.com/AminHP/gym-anytrading/blob/master/gym_anytrading/datasets/data/STOCKS_GOOGL.csv\"\n",
        "\n",
        "    df = pd.read_csv(path, parse_dates=True, index_col=index_name)\n",
        "    return df\n",
        "\n",
        "\n",
        "class Action(int, Enum):\n",
        "    DOUBLE_SELL = 0\n",
        "    SELL = 1\n",
        "    HOLD = 2\n",
        "    BUY = 3\n",
        "    DOUBLE_BUY = 4\n",
        "\n",
        "\n",
        "class Position(int, Enum):\n",
        "    SHORT = -1.\n",
        "    FLAT = 0.\n",
        "    LONG = 1.\n",
        "\n",
        "\n",
        "@dataclasses.dataclass\n",
        "class State:\n",
        "    history: np.ndarray\n",
        "    position_history: List[Position]\n",
        "    tick: float\n",
        "\n",
        "\n",
        "def transform(position: Position, action: Action) -> Any:\n",
        "    \"\"\"\n",
        "    Overview:\n",
        "        used by environment.step().\n",
        "        This func is used to transform the environment's position from\n",
        "        the input (position, action) pair according to the status machine.\n",
        "    Arguments:\n",
        "        - position(Positions) : Long, Short or Flat\n",
        "        - action(int) : Double_Sell, Sell, Hold, Buy, Double_Buy\n",
        "    Returns:\n",
        "        - next_position(Positions) : the position after transformation.\n",
        "    \"\"\"\n",
        "    if action == Action.SELL:\n",
        "        if position == Position.LONG:\n",
        "            return Position.FLAT, False\n",
        "\n",
        "        if position == Position.FLAT:\n",
        "            return Position.SHORT, True\n",
        "\n",
        "    if action == Action.BUY:\n",
        "        if position == Position.SHORT:\n",
        "            return Position.FLAT, False\n",
        "\n",
        "        if position == Position.FLAT:\n",
        "            return Position.LONG, True\n",
        "\n",
        "    if action == Action.DOUBLE_SELL and (position == Position.LONG or position == Position.FLAT):\n",
        "        return Position.SHORT, True\n",
        "\n",
        "    if action == Action.DOUBLE_BUY and (position == Position.SHORT or position == Position.FLAT):\n",
        "        return Position.LONG, True\n",
        "\n",
        "    return position, False\n",
        "\n",
        "\n",
        "class StocksEnv:\n",
        "    def __init__(self, cfg):\n",
        "        self._cfg = cfg\n",
        "        self._env_id = cfg.env_id\n",
        "        # ======== param to plot =========\n",
        "        self.cnt = 0\n",
        "\n",
        "        if 'plot_freq' not in self._cfg:\n",
        "            self.plot_freq = 10\n",
        "        else:\n",
        "            self.plot_freq = self._cfg.plot_freq\n",
        "        if 'save_path' not in self._cfg:\n",
        "            self.save_path = 'data/plots/'\n",
        "        else:\n",
        "            self.save_path = self._cfg.save_path\n",
        "        # ================================\n",
        "\n",
        "        self.train_range = cfg.train_range\n",
        "        self.test_range = cfg.test_range\n",
        "        self.window_size = cfg.window_size\n",
        "        self.prices = None\n",
        "        self.signal_features = None\n",
        "        self.feature_dim_len = None\n",
        "        self.shape = (cfg.window_size, 3)\n",
        "\n",
        "        # ======== param about episode =========\n",
        "        self._start_tick = 0\n",
        "        self._end_tick = 0\n",
        "        self._done = None\n",
        "        self._current_tick = None\n",
        "        self._last_trade_tick = None\n",
        "        self._position = None\n",
        "        self._position_history = None\n",
        "        self._total_reward = None\n",
        "        # ======================================\n",
        "\n",
        "        self._init_flag = True\n",
        "        # init the following variables variable at first reset.\n",
        "        self._action_space = None\n",
        "        self._observation_space = None\n",
        "        self._reward_space = None\n",
        "\n",
        "        self._profit_history = [1.]\n",
        "        # ====== load Google stocks data =======\n",
        "        raw_data = load_dataset(self._cfg.stocks_data_filename, 'Date')\n",
        "        self.raw_prices = raw_data.loc[:, 'Close'].to_numpy()\n",
        "        EPS = 1e-10\n",
        "        self.df = deepcopy(raw_data)\n",
        "        if self.train_range is None or self.test_range is None:\n",
        "            self.df = self.df.apply(lambda x: (x - x.mean()) / (x.std() + EPS), axis=0)\n",
        "        else:\n",
        "            boundary = int(len(self.df) * self.train_range)\n",
        "            train_data = raw_data[:boundary].copy()\n",
        "            boundary = int(len(raw_data) * (1 + self.test_range))\n",
        "            test_data = raw_data[boundary:].copy()\n",
        "\n",
        "            train_data = train_data.apply(lambda x: (x - x.mean()) / (x.std() + EPS), axis=0)\n",
        "            test_data = test_data.apply(lambda x: (x - x.mean()) / (x.std() + EPS), axis=0)\n",
        "            self.df.loc[train_data.index, train_data.columns] = train_data\n",
        "            self.df.loc[test_data.index, test_data.columns] = test_data\n",
        "        # ======================================\n",
        "\n",
        "        # set costs\n",
        "        self.trade_fee_bid_percent = 0.01  # unit\n",
        "        self.trade_fee_ask_percent = 0.005  # unit\n",
        "        # self.trade_fee_bid_percent = 0  # unit\n",
        "        # self.trade_fee_ask_percent = 0  # unit\n",
        "\n",
        "    # override\n",
        "    def _process_data(self, start_idx: int = None) -> Any:\n",
        "        \"\"\"\n",
        "        Overview:\n",
        "            used by environment.reset(), process the raw data.\n",
        "        Arguments:\n",
        "            - start_idx (int): the start tick; if None, then randomly select.\n",
        "        Returns:\n",
        "            - prices: the close.\n",
        "            - signal_features: feature map\n",
        "            - feature_dim_len: the dimension length of selected feature\n",
        "        \"\"\"\n",
        "\n",
        "        # ====== build feature map ========\n",
        "        all_feature_name = ['Close', 'Open', 'High', 'Low', 'Adj Close', 'Volume']\n",
        "        all_feature = {k: self.df.loc[:, k].to_numpy() for k in all_feature_name}\n",
        "        # add feature \"Diff\"\n",
        "        prices = self.df.loc[:, 'Close'].to_numpy()\n",
        "        diff = np.insert(np.diff(prices), 0, 0)\n",
        "        all_feature_name.append('Diff')\n",
        "        all_feature['Diff'] = diff\n",
        "        # =================================\n",
        "\n",
        "        # you can select features you want\n",
        "        # selected_feature_name = ['Close', 'Diff', 'Volume']\n",
        "        selected_feature_name = ['Close', 'Open', 'High', 'Low', 'Adj Close', 'Volume']\n",
        "        selected_feature = np.column_stack([all_feature[k] for k in selected_feature_name])\n",
        "        feature_dim_len = len(selected_feature_name)\n",
        "\n",
        "        # validate index\n",
        "        if start_idx is None:\n",
        "            if self.train_range is None or self.test_range is None:\n",
        "                self.start_idx = np.random.randint(self.window_size, len(self.df) - self._cfg.eps_length)\n",
        "            elif self._env_id[-1] == 'e':\n",
        "                boundary = int(len(self.df) * (1 + self.test_range))\n",
        "                assert len(\n",
        "                    self.df) - self._cfg.eps_length > boundary + self.window_size, \"parameter test_range is too large!\"\n",
        "                self.start_idx = np.random.randint(boundary + self.window_size, len(self.df) - self._cfg.eps_length)\n",
        "            else:\n",
        "                boundary = int(len(self.df) * self.train_range)\n",
        "                assert boundary - self._cfg.eps_length > self.window_size, \"parameter test_range is too small!\"\n",
        "                self.start_idx = np.random.randint(self.window_size, boundary - self._cfg.eps_length)\n",
        "        else:\n",
        "            self.start_idx = start_idx\n",
        "\n",
        "        self._start_tick = self.start_idx\n",
        "        self._end_tick = self._start_tick + self._cfg.eps_length - 1\n",
        "\n",
        "        return prices, selected_feature, feature_dim_len\n",
        "\n",
        "    # override\n",
        "    def _calculate_reward(self, action: Action) -> float:\n",
        "        step_reward = 0.\n",
        "        current_price = (self.raw_prices[self._current_tick])\n",
        "        last_trade_price = (self.raw_prices[self._last_trade_tick])\n",
        "        ratio = current_price / last_trade_price\n",
        "        cost = np.log((1 - self.trade_fee_ask_percent) * (1 - self.trade_fee_bid_percent))\n",
        "\n",
        "        if action == Action.BUY and self._position == Position.SHORT:\n",
        "            step_reward = np.log(2 - ratio) + cost\n",
        "\n",
        "        if action == Action.SELL and self._position == Position.LONG:\n",
        "            step_reward = np.log(ratio) + cost\n",
        "\n",
        "        if action == Action.DOUBLE_SELL and self._position == Position.LONG:\n",
        "            step_reward = np.log(ratio) + cost\n",
        "\n",
        "        if action == Action.DOUBLE_BUY and self._position == Position.SHORT:\n",
        "            step_reward = np.log(2 - ratio) + cost\n",
        "\n",
        "        step_reward = float(step_reward)\n",
        "\n",
        "        return step_reward\n",
        "\n",
        "    # override\n",
        "    def max_possible_profit(self) -> float:\n",
        "        current_tick = self._start_tick\n",
        "        last_trade_tick = current_tick - 1\n",
        "        profit = 1.\n",
        "\n",
        "        while current_tick <= self._end_tick:\n",
        "            if self.raw_prices[current_tick] < self.raw_prices[current_tick - 1]:\n",
        "                while (current_tick <= self._end_tick\n",
        "                       and self.raw_prices[current_tick] < self.raw_prices[current_tick - 1]):\n",
        "                    current_tick += 1\n",
        "\n",
        "                current_price = self.raw_prices[current_tick - 1]\n",
        "                last_trade_price = self.raw_prices[last_trade_tick]\n",
        "                tmp_profit = profit * (2 - (current_price / last_trade_price)) * (1 - self.trade_fee_ask_percent\n",
        "                                                                                  ) * (1 - self.trade_fee_bid_percent)\n",
        "                profit = max(profit, tmp_profit)\n",
        "            else:\n",
        "                while (current_tick <= self._end_tick\n",
        "                       and self.raw_prices[current_tick] >= self.raw_prices[current_tick - 1]):\n",
        "                    current_tick += 1\n",
        "\n",
        "                current_price = self.raw_prices[current_tick - 1]\n",
        "                last_trade_price = self.raw_prices[last_trade_tick]\n",
        "                tmp_profit = profit * (current_price / last_trade_price) * (1 - self.trade_fee_ask_percent\n",
        "                                                                            ) * (1 - self.trade_fee_bid_percent)\n",
        "                profit = max(profit, tmp_profit)\n",
        "            last_trade_tick = current_tick - 1\n",
        "\n",
        "        return profit\n",
        "\n",
        "    def seed(self, seed: int, dynamic_seed: bool = True) -> None:\n",
        "        self._seed = seed\n",
        "        self._dynamic_seed = dynamic_seed\n",
        "        np.random.seed(self._seed)\n",
        "        self.np_random, seed = seeding.np_random(seed)\n",
        "\n",
        "    def reset(self, start_idx: int = None) -> State:\n",
        "        self.cnt += 1\n",
        "        self.prices, self.signal_features, self.feature_dim_len = self._process_data(start_idx)\n",
        "        if self._init_flag:\n",
        "            self.shape = (self.window_size, self.feature_dim_len)\n",
        "            self._action_space = spaces.Discrete(len(Action))\n",
        "            self._observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=self.shape, dtype=np.float64)\n",
        "            self._reward_space = gym.spaces.Box(-inf, inf, shape=(1,), dtype=np.float32)\n",
        "            self._init_flag = False\n",
        "        self._done = False\n",
        "        self._current_tick = self._start_tick\n",
        "        self._last_trade_tick = self._current_tick - 1\n",
        "        self._position = Position.FLAT\n",
        "        self._position_history = [self._position]\n",
        "        self._profit_history = [1.]\n",
        "        self._total_reward = 0.\n",
        "\n",
        "        return self._get_observation()\n",
        "\n",
        "    def random_action(self) -> Any:\n",
        "        return np.array([self.action_space.sample()])\n",
        "\n",
        "    def position_history(self, n) -> List[float]:\n",
        "        # Returns the n most recent positions\n",
        "        if n < len(self._position) - 1:\n",
        "            return self._position\n",
        "        else:\n",
        "            return self._position[-n:]\n",
        "\n",
        "    def step(self, action: Action) -> BaseEnvTimestep:\n",
        "        self._done = False\n",
        "        self._current_tick += 1\n",
        "\n",
        "        if self._current_tick >= self._end_tick:\n",
        "            self._done = True\n",
        "\n",
        "        step_reward = self._calculate_reward(action)\n",
        "        self._total_reward += step_reward\n",
        "\n",
        "        self._position, trade = transform(self._position, action)\n",
        "\n",
        "        if trade:\n",
        "            self._last_trade_tick = self._current_tick\n",
        "\n",
        "        self._position_history.append(self._position)\n",
        "        self._profit_history.append(float(np.exp(self._total_reward)))\n",
        "        observation = self._get_observation()\n",
        "        info = dict(\n",
        "            total_reward=self._total_reward,\n",
        "            position=self._position.value,\n",
        "        )\n",
        "\n",
        "        if self._done:\n",
        "            if self._env_id[-1] == 'e' and self.cnt % self.plot_freq == 0:\n",
        "                self.render()\n",
        "            info['max_possible_profit'] = np.log(self.max_possible_profit())\n",
        "            info['final_eval_reward'] = self._total_reward\n",
        "\n",
        "        # return BaseEnvTimestep(observation, step_reward - 1, self._done, info)\n",
        "        return BaseEnvTimestep(observation, self._profit_history[-1] - self._profit_history[-2], self._done, info)\n",
        "\n",
        "    def _get_observation(self) -> State:\n",
        "        obs = np.array(self.signal_features[(self._current_tick - self.window_size + 1):\n",
        "                                            self._current_tick + 1]).astype(np.float32)\n",
        "\n",
        "        tick = (self._current_tick - self._last_trade_tick) / self._cfg.eps_length\n",
        "\n",
        "        return State(history=obs,\n",
        "                     position_history=self._position_history[-min(len(self._position_history), self.window_size):],\n",
        "                     tick=tick)\n",
        "\n",
        "    def render_profit(self, save=False):\n",
        "        plt.clf()\n",
        "        plt.xlabel('trading days')\n",
        "        plt.ylabel('profit')\n",
        "        plt.plot(self._profit_history)\n",
        "        if save:\n",
        "            plt.savefig(self.save_path + str(self._env_id) + \"-profit.png\")\n",
        "        else:\n",
        "            plt.show()\n",
        "\n",
        "    def render_price(self, save=False):\n",
        "        plt.clf()\n",
        "        plt.xlabel('trading days')\n",
        "        plt.ylabel('close price')\n",
        "        window_ticks = np.arange(len(self._position_history))\n",
        "        eps_price = self.raw_prices[self._start_tick:self._end_tick + 1]\n",
        "        plt.plot(eps_price)\n",
        "\n",
        "        short_ticks = []\n",
        "        long_ticks = []\n",
        "        flat_ticks = []\n",
        "        for i, tick in enumerate(window_ticks):\n",
        "            if self._position_history[i] == Position.SHORT:\n",
        "                short_ticks.append(tick)\n",
        "            elif self._position_history[i] == Position.LONG:\n",
        "                long_ticks.append(tick)\n",
        "            else:\n",
        "                flat_ticks.append(tick)\n",
        "\n",
        "        plt.plot(long_ticks, eps_price[long_ticks], 'g^', markersize=3, label=\"Long\")\n",
        "        plt.plot(flat_ticks, eps_price[flat_ticks], 'bo', markersize=3, label=\"Flat\")\n",
        "        plt.plot(short_ticks, eps_price[short_ticks], 'rv', markersize=3, label=\"Short\")\n",
        "        plt.legend(loc='upper left', bbox_to_anchor=(0.05, 0.95))\n",
        "        if save:\n",
        "            plt.savefig(self.save_path + str(self._env_id) + '-price.png')\n",
        "        else:\n",
        "            plt.show()\n",
        "\n",
        "    def final_profit(self) -> float:\n",
        "        return self._profit_history[-1]\n",
        "\n",
        "    def render(self, save=True) -> None:\n",
        "        self.render_profit(save)\n",
        "        self.render_price(save)\n",
        "\n",
        "    def render_together(self, save=True, filename: str = None) -> None:\n",
        "        fig, axs = plt.subplots(2)\n",
        "\n",
        "        axs[0].set_xlabel('trading days')\n",
        "        axs[0].set_ylabel('close price')\n",
        "        window_ticks = np.arange(len(self._position_history))\n",
        "        eps_price = self.raw_prices[self._start_tick:self._end_tick + 1]\n",
        "        axs[0].plot(eps_price)\n",
        "\n",
        "        short_ticks = []\n",
        "        long_ticks = []\n",
        "        flat_ticks = []\n",
        "        for i, tick in enumerate(window_ticks):\n",
        "            if self._position_history[i] == Position.SHORT:\n",
        "                short_ticks.append(tick)\n",
        "            elif self._position_history[i] == Position.LONG:\n",
        "                long_ticks.append(tick)\n",
        "            else:\n",
        "                flat_ticks.append(tick)\n",
        "\n",
        "        axs[0].plot(long_ticks, eps_price[long_ticks], 'g^', markersize=3, label=\"Long\")\n",
        "        axs[0].plot(flat_ticks, eps_price[flat_ticks], 'bo', markersize=3, label=\"Flat\")\n",
        "        axs[0].plot(short_ticks, eps_price[short_ticks], 'rv', markersize=3, label=\"Short\")\n",
        "        axs[0].legend(loc='upper left', bbox_to_anchor=(0.05, 0.95))\n",
        "\n",
        "        # profit history below\n",
        "        axs[1].set_xlabel('trading days')\n",
        "        axs[1].set_ylabel('profit')\n",
        "        axs[1].plot(self._profit_history)\n",
        "\n",
        "        if not filename:\n",
        "            filename = self.save_path + str(self._env_id) + '-price-profit.png'\n",
        "\n",
        "        Path(filename[:filename.rindex('/')]).mkdir(parents=True, exist_ok=True)\n",
        "        plt.savefig(filename)\n",
        "\n",
        "    def close(self):\n",
        "        plt.close()\n",
        "\n",
        "    @property\n",
        "    def observation_space(self) -> gym.spaces.Space:\n",
        "        return self._observation_space\n",
        "\n",
        "    @property\n",
        "    def action_space(self) -> gym.spaces.Space:\n",
        "        return self._action_space\n",
        "\n",
        "    @property\n",
        "    def reward_space(self) -> gym.spaces.Space:\n",
        "        return self._reward_space\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return \"DI-engine Stocks Trading Env\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utility Functions"
      ],
      "metadata": {
        "collapsed": false,
        "id": "fUTEA6-9gQg0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "outputs": [],
      "source": [
        "def plot_curves(arr_list, legend_list, color_list, ylabel, fig_title):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        arr_list (list): list of results arrays to plot\n",
        "        legend_list (list): list of legends corresponding to each result array\n",
        "        color_list (list): list of color corresponding to each result array\n",
        "        ylabel (string): label of the Y axis\n",
        "\n",
        "        Note that, make sure the elements in the arr_list, legend_list and color_list are associated with each other correctly.\n",
        "        Do not forget to change the ylabel for different plots.\n",
        "    \"\"\"\n",
        "    # set the figure type\n",
        "    fig, ax = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "    # PLEASE NOTE: Change the labels for different plots\n",
        "    ax.set_ylabel(ylabel)\n",
        "    ax.set_xlabel(\"Time Steps\")\n",
        "\n",
        "    # plot results\n",
        "    h_list = []\n",
        "    for arr, legend, color in zip(arr_list, legend_list, color_list):\n",
        "        # compute the standard error\n",
        "        arr_err = arr.std(axis=0) / np.sqrt(arr.shape[0])\n",
        "        # plot the mean\n",
        "        h, = ax.plot(range(arr.shape[1]), arr.mean(axis=0), color=color, label=legend)\n",
        "        # plot the confidence band\n",
        "        arr_err *= 1.96\n",
        "        ax.fill_between(range(arr.shape[1]), arr.mean(axis=0) - arr_err, arr.mean(axis=0) + arr_err, alpha=0.3,\n",
        "                        color=color)\n",
        "        # save the plot handle\n",
        "        h_list.append(h)\n",
        "\n",
        "    # plot legends\n",
        "    ax.set_title(f\"{fig_title}\")\n",
        "    ax.legend(handles=h_list)"
      ],
      "metadata": {
        "id": "QEzcv0wWgQg0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [],
      "source": [
        "import time\n",
        "from dateutil import tz\n",
        "from typing import Optional\n",
        "import dill as pickle\n",
        "\n",
        "import datetime\n",
        "\n",
        "\n",
        "@dataclasses.dataclass\n",
        "class ExperimentResult:\n",
        "    config: Any\n",
        "    final_env: Optional[StocksEnv]\n",
        "    profits: List[float]\n",
        "    returns: Optional[List[float]]\n",
        "    loss: Optional[List[float]]\n",
        "    max_possible_profits: Optional[List[float]]\n",
        "    buy_and_hold_profits: Optional[List[float]]\n",
        "    algorithm: str\n",
        "    timestamp: str = dataclasses.field(init=False)\n",
        "\n",
        "    def __post_init__(self):\n",
        "        self.timestamp = datetime.datetime.utcfromtimestamp(time.time()).replace(tzinfo=tz.gettz('UTC')).astimezone(\n",
        "            tz=tz.gettz('America/Boston')).strftime('%Y-%m-%d--%H-%M-%S')\n",
        "\n",
        "    def to_file(self):\n",
        "        with open(self.pickle_filename(), 'wb') as f:\n",
        "            pickle.dump(self, f)\n",
        "            return self.pickle_filename()\n",
        "\n",
        "    def pickle_filename(self) -> str:\n",
        "        return f'{self.algorithm}_{self.timestamp}.pickle'\n",
        "\n",
        "    def name(self) -> str:\n",
        "        return f'{self.algorithm}_{self.timestamp}'\n",
        "\n",
        "    @staticmethod\n",
        "    def from_file(path: str) -> 'ExperimentResult':\n",
        "        with open(path, 'rb') as handle:\n",
        "            return pickle.load(handle)"
      ],
      "metadata": {
        "id": "vbMOkIRrgQg1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DQN Agent Definition"
      ],
      "metadata": {
        "collapsed": false,
        "id": "dlKSxdOTgQg1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "outputs": [],
      "source": [
        "from torch.nn import Conv2d, Flatten, LazyLinear, Linear, MaxPool2d, ReLU\n",
        "import torch\n",
        "import tqdm\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "# customized weight initialization\n",
        "def customized_weights_init(m):\n",
        "    # compute the gain\n",
        "    gain = nn.init.calculate_gain('relu')\n",
        "    # init the convolutional layer\n",
        "    if isinstance(m, nn.Conv2d):\n",
        "        # init the params using uniform\n",
        "        nn.init.xavier_uniform_(m.weight, gain=gain)\n",
        "        nn.init.constant_(m.bias, 0)\n",
        "    # init the linear layer\n",
        "    if isinstance(m, nn.Linear):\n",
        "        # init the params using uniform\n",
        "        nn.init.xavier_uniform_(m.weight, gain=gain)\n",
        "        nn.init.constant_(m.bias, 0)\n",
        "\n",
        "\n",
        "class DeepQNet(nn.Module):\n",
        "    def __init__(self, input_dim, num_hidden_layer, dim_hidden_layer, output_dim):\n",
        "        super(DeepQNet, self).__init__()\n",
        "\n",
        "        \"\"\"CODE HERE: construct your Deep neural network\n",
        "        \"\"\"\n",
        "\n",
        "        self.layers = nn.ModuleList()\n",
        "\n",
        "        self.layers.append(Conv2d(in_channels=1, out_channels=1, kernel_size=(3, 1)))\n",
        "        self.layers.append(ReLU())\n",
        "        # self.layers.append(MaxPool2d(kernel_size=(4, 1), stride=(1, 1)))\n",
        "\n",
        "        self.layers.append(Conv2d(in_channels=1, out_channels=1, kernel_size=(3, 1)))\n",
        "        self.layers.append(ReLU())\n",
        "        # self.layers.append(MaxPool2d(kernel_size=(4, 1), stride=(2, 1)))\n",
        "\n",
        "        self.layers.append(Conv2d(in_channels=1, out_channels=1, kernel_size=(3, 1)))\n",
        "        self.layers.append(ReLU())\n",
        "        # self.layers.append(MaxPool2d(kernel_size=(4, 1), stride=(2, 1)))\n",
        "\n",
        "        self.layers.append(Conv2d(in_channels=1, out_channels=1, kernel_size=(5, 1)))\n",
        "        self.layers.append(ReLU())\n",
        "        # self.layers.append(MaxPool2d(kernel_size=(8, 1), stride=(2, 1)))\n",
        "\n",
        "        self.layers.append(Flatten())\n",
        "        self.layers.append(LazyLinear(out_features=64))\n",
        "        self.layers.append(ReLU())\n",
        "\n",
        "        self.layers.append(LazyLinear(out_features=5))\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"CODE HERE: implement your forward propagation\n",
        "        \"\"\"\n",
        "        for idx, layer in enumerate(self.layers):\n",
        "            # print(f'On layer {idx}({layer.__class__}) with input shape {x.size()}')\n",
        "            x = layer(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ReplayBuffer(object):\n",
        "    \"\"\" Implement the Replay Buffer as a class, which contains:\n",
        "            - self._data_buffer (list): a list variable to store all transition tuples.\n",
        "            - add: a function to add new transition tuple into the buffer\n",
        "            - sample_batch: a function to sample a batch training data from the Replay Buffer\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, buffer_size):\n",
        "        \"\"\"Args:\n",
        "               buffer_size (int): size of the replay buffer\n",
        "        \"\"\"\n",
        "        # total size of the replay buffer\n",
        "        self.total_size = buffer_size\n",
        "\n",
        "        # create a list to store the transitions\n",
        "        self._data_buffer = []\n",
        "        self._next_idx = 0\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._data_buffer)\n",
        "\n",
        "    def add(self, obs, act, reward, next_obs, done):\n",
        "        # create a tuple\n",
        "        trans = (obs, act, reward, next_obs, done)\n",
        "\n",
        "        # interesting implementation\n",
        "        if self._next_idx >= len(self._data_buffer):\n",
        "            self._data_buffer.append(trans)\n",
        "        else:\n",
        "            self._data_buffer[self._next_idx] = trans\n",
        "\n",
        "        # increase the index\n",
        "        self._next_idx = (self._next_idx + 1) % self.total_size\n",
        "\n",
        "    def _encode_sample(self, indices):\n",
        "        \"\"\" Function to fetch the state, action, reward, next state, and done arrays.\n",
        "\n",
        "            Args:\n",
        "                indices (list): list contains the index of all sampled transition tuples.\n",
        "        \"\"\"\n",
        "        # lists for transitions\n",
        "        obs_list, actions_list, rewards_list, next_obs_list, dones_list = [], [], [], [], []\n",
        "\n",
        "        # collect the data\n",
        "        for idx in indices:\n",
        "            # get the single transition\n",
        "            data = self._data_buffer[idx]\n",
        "            obs, act, reward, next_obs, d = data\n",
        "            # store to the list\n",
        "            obs_list.append(np.array(obs, copy=False))\n",
        "            actions_list.append(np.array(act, copy=False))\n",
        "            rewards_list.append(np.array(reward, copy=False))\n",
        "            next_obs_list.append(np.array(next_obs, copy=False))\n",
        "            dones_list.append(np.array(d, copy=False))\n",
        "        # return the sampled batch data as numpy arrays\n",
        "        return np.array(obs_list), np.array(actions_list), np.array(rewards_list), np.array(next_obs_list), np.array(\n",
        "            dones_list)\n",
        "\n",
        "    def sample_batch(self, batch_size):\n",
        "        \"\"\" Args:\n",
        "                batch_size (int): size of the sampled batch data.\n",
        "        \"\"\"\n",
        "        # sample indices with replaced\n",
        "        indices = [np.random.randint(0, len(self._data_buffer)) for _ in range(batch_size)]\n",
        "        return self._encode_sample(indices)\n",
        "\n",
        "\n",
        "class LinearSchedule(object):\n",
        "    \"\"\" This schedule returns the value linearly\"\"\"\n",
        "\n",
        "    def __init__(self, start_value, end_value, duration):\n",
        "        # start value\n",
        "        self._start_value = start_value\n",
        "        # end value\n",
        "        self._end_value = end_value\n",
        "        # time steps that value changes from the start value to the end value\n",
        "        self._duration = duration\n",
        "        # difference between the start value and the end value\n",
        "        self._schedule_amount = end_value - start_value\n",
        "\n",
        "    def get_value(self, time):\n",
        "        # logic: if time > duration, use the end value, else use the scheduled value\n",
        "        \"\"\" CODE HERE: return the epsilon for each time step within the duration.\n",
        "        \"\"\"\n",
        "        if time > self._duration:\n",
        "            return self._end_value\n",
        "        else:\n",
        "            return self._start_value + (time / self._duration * (self._end_value - self._start_value))\n",
        "\n",
        "\n",
        "class DQNAgent(object):\n",
        "    # initialize the agent\n",
        "    def __init__(self,\n",
        "                 params,\n",
        "                 ):\n",
        "        # save the parameters\n",
        "        self.params = params\n",
        "\n",
        "        # environment parameters\n",
        "        self.action_dim = params['action_dim']\n",
        "        self.obs_dim = params['observation_dim']\n",
        "\n",
        "        # executable actions\n",
        "        self.action_space = params['action_space']\n",
        "\n",
        "        # create behavior policy network\n",
        "        self.behavior_policy_net = DeepQNet(input_dim=params['observation_dim'],\n",
        "                                            num_hidden_layer=params['hidden_layer_num'],\n",
        "                                            dim_hidden_layer=params['hidden_layer_dim'],\n",
        "                                            output_dim=params['action_dim'])\n",
        "        # create target network\n",
        "        self.target_policy_net = DeepQNet(input_dim=params['observation_dim'],\n",
        "                                          num_hidden_layer=params['hidden_layer_num'],\n",
        "                                          dim_hidden_layer=params['hidden_layer_dim'],\n",
        "                                          output_dim=params['action_dim'])\n",
        "\n",
        "        # initialize target network with behavior network\n",
        "        # self.behavior_policy_net.apply(customized_weights_init)  # TODO: should custom initialization?\n",
        "        # self.target_policy_net.load_state_dict(self.behavior_policy_net.state_dict())\n",
        "\n",
        "        # send the agent to a specific device: cpu or gpu\n",
        "        # self.device = torch.device(\"cpu\")\n",
        "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.behavior_policy_net.to(self.device)\n",
        "        self.target_policy_net.to(self.device)\n",
        "\n",
        "        # optimizer\n",
        "        self.optimizer = torch.optim.Adam(self.behavior_policy_net.parameters(), lr=params['learning_rate'])\n",
        "\n",
        "    # get action\n",
        "    def get_action(self, obs, eps):\n",
        "        if np.random.random() < eps:  # with probability eps, the agent selects a random action\n",
        "            action = self.action_space.sample()\n",
        "        else:  # with probability 1 - eps, the agent selects a greedy policy\n",
        "            obs = self._arr_to_tensor(obs)\n",
        "            # obs = self._arr_to_tensor(obs).view(1, -1)\n",
        "            with torch.no_grad():\n",
        "                q_values = self.behavior_policy_net(obs)\n",
        "                action = q_values.max(dim=1)[1].item()\n",
        "        return action\n",
        "\n",
        "    # update behavior policy\n",
        "    def update_behavior_policy(self, batch_data, timestep):\n",
        "        # convert batch data to tensor and put them on device\n",
        "        batch_data_tensor = self._batch_to_tensor(batch_data)\n",
        "\n",
        "        # get the transition data\n",
        "        obs_tensor = batch_data_tensor['obs']\n",
        "        actions_tensor = batch_data_tensor['action']\n",
        "        next_obs_tensor = batch_data_tensor['next_obs']\n",
        "        rewards_tensor = batch_data_tensor['reward']\n",
        "        dones_tensor = batch_data_tensor['done']\n",
        "\n",
        "        \"\"\"CODE HERE:\n",
        "                Compute the predicted Q values using the behavior policy network\n",
        "        \"\"\"\n",
        "        q_estimate = self.behavior_policy_net(obs_tensor).gather(dim=1, index=actions_tensor).flatten()\n",
        "        with torch.no_grad():\n",
        "            q_max = torch.max(self.target_policy_net(next_obs_tensor), dim=1).values\n",
        "\n",
        "            td_target = rewards_tensor.flatten() + self.params['gamma'] * q_max\n",
        "            done_idxs = (dones_tensor == 1).nonzero(as_tuple=True)[0]\n",
        "            td_target[done_idxs] = rewards_tensor[done_idxs, 0]\n",
        "\n",
        "        loss_accumulation_steps = 200\n",
        "\n",
        "        # compute the loss\n",
        "        td_loss = torch.nn.MSELoss()(q_estimate, td_target)\n",
        "        td_loss.backward()\n",
        "\n",
        "        # minimize the loss\n",
        "        self.optimizer.step()\n",
        "        self.behavior_policy_net.zero_grad()\n",
        "\n",
        "        return td_loss.item()\n",
        "\n",
        "    # update update target policy\n",
        "    def update_target_policy(self):\n",
        "        # hard update\n",
        "        \"\"\"CODE HERE:\n",
        "                Copy the behavior policy network to the target network\n",
        "        \"\"\"\n",
        "        self.target_policy_net.load_state_dict(self.behavior_policy_net.state_dict())\n",
        "\n",
        "    # load trained model\n",
        "    def load_model(self, model_file):\n",
        "        # load the trained model\n",
        "        self.behavior_policy_net.load_state_dict(torch.load(model_file, map_location=self.device))\n",
        "        self.behavior_policy_net.eval()\n",
        "\n",
        "    # auxiliary functions\n",
        "    def _arr_to_tensor(self, arr):\n",
        "        arr = np.array(arr).reshape((-1, 1, arr.shape[0], arr.shape[1]))\n",
        "        arr_tensor = torch.from_numpy(arr).float().to(self.device)\n",
        "        return arr_tensor\n",
        "\n",
        "    def _batch_to_tensor(self, batch_data):\n",
        "        # store the tensor\n",
        "        batch_data_tensor = {'obs': [], 'action': [], 'reward': [], 'next_obs': [], 'done': []}\n",
        "        # get the numpy arrays\n",
        "        obs_arr, action_arr, reward_arr, next_obs_arr, done_arr = batch_data\n",
        "        # convert to tensors\n",
        "        batch_data_tensor['obs'] = torch.as_tensor(obs_arr, dtype=torch.float32, device=self.device).reshape(\n",
        "            (-1, 1, obs_arr.shape[1], obs_arr.shape[2]))  # for CNN\n",
        "        batch_data_tensor['action'] = torch.as_tensor(action_arr, device=self.device).long().view(-1, 1)\n",
        "        batch_data_tensor['reward'] = torch.as_tensor(reward_arr, dtype=torch.float32, device=self.device).view(-1, 1)\n",
        "        batch_data_tensor['next_obs'] = torch.as_tensor(next_obs_arr, dtype=torch.float32, device=self.device).reshape(\n",
        "            (-1, 1, obs_arr.shape[1], obs_arr.shape[2]))\n",
        "        batch_data_tensor['done'] = torch.as_tensor(done_arr, dtype=torch.float32, device=self.device).view(-1, 1)\n",
        "\n",
        "        return batch_data_tensor\n",
        "\n",
        "\n",
        "def train_dqn_agent(env, params):\n",
        "    # create the DQN agent\n",
        "    my_agent = DQNAgent(params)\n",
        "\n",
        "    # create the epsilon-greedy schedule\n",
        "    my_schedule = LinearSchedule(start_value=params['epsilon_start_value'],\n",
        "                                 end_value=params['epsilon_end_value'],\n",
        "                                 duration=params['epsilon_duration'])\n",
        "\n",
        "    # create the replay buffer\n",
        "    replay_buffer = ReplayBuffer(params['replay_buffer_size'])\n",
        "\n",
        "    # training variables\n",
        "    episode_t = 0\n",
        "    rewards = []\n",
        "    train_returns = []\n",
        "    train_loss = []\n",
        "    profits = []\n",
        "\n",
        "    # reset the environment\n",
        "    obs = env.reset()\n",
        "\n",
        "    num_episodes = params['total_training_time_step'] // params['max_time_step_per_episode']\n",
        "\n",
        "    # start training\n",
        "    pbar = tqdm.trange(params['total_training_time_step'])\n",
        "    last_best_return = np.negative(np.inf)\n",
        "    for t in pbar:\n",
        "        # scheduled epsilon at time step t\n",
        "        eps_t = my_schedule.get_value(t)\n",
        "        # get one epsilon-greedy action\n",
        "        action = my_agent.get_action(obs, eps_t)\n",
        "\n",
        "        # step in the environment\n",
        "        next_obs, reward, done, _ = env.step(action)\n",
        "\n",
        "        # add to the buffer\n",
        "        replay_buffer.add(obs, action, reward, next_obs, done)\n",
        "        rewards.append(reward)\n",
        "\n",
        "        # check termination\n",
        "        if done or episode_t == params['max_time_step_per_episode'] - 1:\n",
        "            # compute the return\n",
        "            G = 0\n",
        "            for r in reversed(rewards):\n",
        "                G = r + params['gamma'] * G\n",
        "\n",
        "            # if G > last_best_return:\n",
        "            #     torch.save(my_agent.behavior_policy_net.state_dict(), f\"./{params['model_name']}\")\n",
        "\n",
        "            # store the return\n",
        "            train_returns.append(G)\n",
        "            episode_idx = len(train_returns)\n",
        "\n",
        "            # print the information\n",
        "            pbar.set_description(\n",
        "                f\"Ep={episode_idx} | \"\n",
        "                f\"G={np.mean(train_returns[-10:]) if train_returns else 0:.2f} | \"\n",
        "                f\"Eps={eps_t:.5f}\"\n",
        "            )\n",
        "\n",
        "            # plot the last x episodes\n",
        "            # if episode_idx == 1 or episode_idx > num_episodes - params['final_policy_num_plots']:\n",
        "            #     env.render_together(save=True, filename=f'data/plots/dqn2_{params[\"name\"]}/episode_{episode_idx}')\n",
        "\n",
        "            # reset the environment\n",
        "            episode_t, rewards = 0, []\n",
        "            profits.append(env.final_profit())\n",
        "            env.close()\n",
        "            obs = env.reset()\n",
        "        else:\n",
        "            # increment\n",
        "            obs = next_obs\n",
        "            episode_t += 1\n",
        "\n",
        "        if t > params['start_training_step']:\n",
        "            # update the behavior model\n",
        "            if not np.mod(t, params['freq_update_behavior_policy']):\n",
        "                \"\"\" CODE HERE:\n",
        "                    Update the behavior policy network\n",
        "                \"\"\"\n",
        "                train_loss.append(my_agent.update_behavior_policy(replay_buffer.sample_batch(params['batch_size']), t))\n",
        "\n",
        "            # update the target model\n",
        "            if not np.mod(t, params['freq_update_target_policy']):\n",
        "                \"\"\" CODE HERE:\n",
        "                    Update the target policy network\n",
        "                \"\"\"\n",
        "                my_agent.update_target_policy()\n",
        "\n",
        "    # save the results\n",
        "    return train_returns, train_loss, profits"
      ],
      "metadata": {
        "id": "ypaAlElVgQg2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
            "  warnings.warn('Lazy modules are a new feature under heavy development '\n",
            "Ep=1409 | G=-0.00 | Eps=0.00000:  56%|█████▌    | 280354/500000 [02:58<03:11, 1148.46it/s]"
          ]
        }
      ],
      "source": [
        "from easydict import EasyDict\n",
        "import random\n",
        "\n",
        "\n",
        "class StocksEnvWithFeatureVectors(StocksEnv):\n",
        "\n",
        "    def _get_observation(self) -> np.ndarray:\n",
        "        observation = super()._get_observation()\n",
        "        position_history = np.zeros(shape=(observation.history.shape[0], 1))\n",
        "        position_history[-len(observation.position_history):, 0] = observation.position_history\n",
        "\n",
        "        # gives a warning because it's not a state but shut the up\n",
        "        # align the position history with where the stock was at that point\n",
        "        return np.hstack([observation.history, position_history])\n",
        "\n",
        "name='altogether_one_minus_reward'\n",
        "\n",
        "np.random.seed(1234)\n",
        "random.seed(1234)\n",
        "torch.manual_seed(1234)\n",
        "\n",
        "env = StocksEnvWithFeatureVectors(EasyDict({\n",
        "    \"env_id\": 'stocks-dqn', \"eps_length\": 200,\n",
        "    \"window_size\": 200, \"train_range\": None, \"test_range\": None,\n",
        "    \"stocks_data_filename\": 'STOCKS_GOOGL'\n",
        "}))\n",
        "\n",
        "initial_obs = env.reset()\n",
        "\n",
        "# create training parameters\n",
        "train_parameters = {\n",
        "    'observation_dim': initial_obs.shape,\n",
        "    'action_dim': 5,\n",
        "    'action_space': env.action_space,\n",
        "    'hidden_layer_num': 16,\n",
        "    'hidden_layer_dim': 32,\n",
        "    'gamma': 0.99,\n",
        "\n",
        "    'max_time_step_per_episode': 500,\n",
        "\n",
        "    'total_training_time_step': 5_000_000 // 10,\n",
        "\n",
        "    'epsilon_start_value': 1.0,\n",
        "    'epsilon_end_value': 0.0,\n",
        "    'epsilon_duration': 2_500_000 // 10,\n",
        "\n",
        "    'replay_buffer_size': 500_000,\n",
        "    'start_training_step': 20_000,\n",
        "    'freq_update_behavior_policy': 40,\n",
        "    'freq_update_target_policy': 20_000,\n",
        "\n",
        "    'batch_size': 64,\n",
        "    'learning_rate': 1e-3,\n",
        "\n",
        "    'final_policy_num_plots': 20,\n",
        "\n",
        "    'model_name': \"stocks_google.pt\",\n",
        "    'name': name\n",
        "}\n",
        "\n",
        "# create experiment\n",
        "train_returns, train_loss, train_profits = train_dqn_agent(env, train_parameters)\n",
        "plot_curves([np.array([train_returns])], ['dqn'], ['r'], 'discounted return', 'DQN2')\n",
        "plt.savefig(f'dqn2_returns_{name}')\n",
        "plt.clf()\n",
        "plot_curves([np.array([train_loss])], ['dqn'], ['r'], 'training loss', 'DQN2')\n",
        "plt.savefig(f'dqn2_loss_{name}')\n",
        "plt.clf()\n",
        "plot_curves([np.array([train_profits])], ['dqn'], ['r'], 'profit', 'DQN2')\n",
        "plt.savefig(f'dqn2_profits_{name}')\n",
        "\n",
        "ExperimentResult(\n",
        "    config=train_parameters,\n",
        "    final_env=None,\n",
        "    profits=train_profits,\n",
        "    returns=train_returns,\n",
        "    loss=train_loss,\n",
        "    max_possible_profits=None,\n",
        "    buy_and_hold_profits=None,\n",
        "    algorithm=f'dqn2_{name}'\n",
        ").to_file()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HlZMuyi3gQg4",
        "outputId": "361456e5-369b-4afe-bef1-72bc430bc4e9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def moving_average(a, n=3) :\n",
        "    ret = np.cumsum(a, dtype=float)\n",
        "    ret[n:] = ret[n:] - ret[:-n]\n",
        "    return ret[n - 1:] / n\n",
        "\n",
        "profit_moving_avg = moving_average(train_profits, n=50)"
      ],
      "metadata": {
        "id": "3cS6AbcOlMHW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_curves([np.array([train_profits]), np.array([profit_moving_avg])], ['raw profits', '50-episode moving average'], ['r', 'g'], 'profit', 'DQN2')\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "68sEv8IicEFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.mean(np.array(train_profits[15000:]))"
      ],
      "metadata": {
        "id": "ICKaM0Q5cHtZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EyZO0w2Bc2wZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}